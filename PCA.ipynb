{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Palpax Online Assessment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRJ64RFSVx7D1iQ9W38NcX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASHwaniC/Machine-Learning-Algorithm-Codes/blob/main/PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0klMQ0DIS7tp"
      },
      "source": [
        "PCA\n",
        "\n",
        "PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvEICt8Jznx"
      },
      "source": [
        "class PCA:\n",
        "  def __init__(self, n_components):\n",
        "    self.n_components = n_components\n",
        "    self.components = None\n",
        "    self.mean = None\n",
        "\n",
        "  def fit(self, X):\n",
        "    #mean\n",
        "    self.mean = np.mean(X, axis=0)\n",
        "    #1 row = sample, columns = features\n",
        "    X = X - self.mean\n",
        "\n",
        "    #covariance\n",
        "    #but in documentation 1row = features, columns=samples for cov ...so we transpose our data\n",
        "    cov = np.cov(X.T)\n",
        "\n",
        "    #eigenvectors, eigenvalues\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
        "\n",
        "    #v[:, i]\n",
        "    #sort eigenvectors\n",
        "    eigenvectors = eigenvectors.T\n",
        "    idxs = np.argsort(eigenvalues)[::-1]\n",
        "    \n",
        "    eigenvalues = eigenvalues[idxs]\n",
        "    eigenvectors = eigenvectors[idxs]\n",
        "\n",
        "    #store first n eigenvectors\n",
        "    self.components = eigenvectors[0:self.n_components]\n",
        "\n",
        "  def transform(self, X):\n",
        "    #project data\n",
        "    X = X - self.mean\n",
        "    return(np.dot(X, self.components.T))\n",
        "\n",
        "#PCA TEST\n",
        "data = datasets.load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "#project data onto the 2 primary principal components\n",
        "pca = PCA(2)\n",
        "pca.fit(X)\n",
        "X_projected = pca.transform(X)\n",
        "\n",
        "\n",
        "\n",
        "x1 = X_projected[:,0]\n",
        "x2 = X_projected[:,1]\n",
        "\n",
        "plt.scatter(x1,x2,c=y,edgecolor='none',alpha=0.8,cmap=plt.cm.get_cmap('viridis',3))\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}