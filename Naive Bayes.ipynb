{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Palpax Online Assessment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPofWyPHUJ6NBikYlmpz2lO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASHwaniC/Machine-Learning-Algorithm-Codes/blob/main/Naive%20Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0klMQ0DIS7tp"
      },
      "source": [
        "Naive Bayes \n",
        "\n",
        "In machine learning, naïve Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. They are among the simplest Bayesian network models.But they could be coupled with Kernel density estimation and achieve higher accuracy levels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvEICt8Jznx"
      },
      "source": [
        "class NaiveBayes:\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    self._classes = np.unique(y)\n",
        "    n_classes = len(self._classes)\n",
        "\n",
        "    #init mean, var, priors\n",
        "    self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "    self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "    self._priors = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "    for c in self._classes:\n",
        "      X_c = X[c==y]\n",
        "      self._mean[c,:]=X_c.mean(axis=0)\n",
        "      self._var[c,:]=X_c.var(axis=0)\n",
        "      self._priors[c] = X_c.shape[0] / float(n_samples)\n",
        "\n",
        "  def predict(self,X):\n",
        "    y_pred = [self._predict(x) for x in X]\n",
        "    return(y_pred)\n",
        "    \n",
        "  def _predict(self, x):\n",
        "    posteriors = []\n",
        "    for idx, c in enumerate(self._classes):\n",
        "      #print(idx,c)\n",
        "      prior = np.log(self._priors[idx])\n",
        "      class_conditional = np.sum(np.log(self._pdf(idx,x)))\n",
        "      posterior = prior + class_conditional\n",
        "      posteriors.append(posterior)\n",
        "    return(self._classes[np.argmax(posteriors)])\n",
        "\n",
        "\n",
        "    \n",
        "  def _pdf(self, class_idx, x):\n",
        "    mean = self._mean[class_idx]\n",
        "    var = self._var[class_idx]\n",
        "    numerator = np.exp(-(x-mean)**2/(2 * var))\n",
        "    denominator = np.sqrt(2*np.pi*var)\n",
        "    return(numerator/denominator)\n",
        "\n",
        "\n",
        "# Naive Bayes Test\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  accuracy = np.sum(y_true == y_pred)/len(y_true)\n",
        "  return(accuracy)\n",
        "\n",
        "X,y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "nb = NaiveBayes()\n",
        "nb.fit(X_train, y_train)\n",
        "predictions = nb.predict(X_test)\n",
        "\n",
        "print(\"Naive Bayes classification accuracy \",accuracy(y_test, predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}