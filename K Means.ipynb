{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Palpax Online Assessment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzkZREjyUtZdsSE1AXWXxg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASHwaniC/Machine-Learning-Algorithm-Codes/blob/main/K%20Means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0klMQ0DIS7tp"
      },
      "source": [
        "K Means\n",
        "\n",
        "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition and observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. It is popular for cluster analysis in data mining. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, Better Euclidean solutions can be found using k-medians and k-medoids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvEICt8Jznx"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X, y = make_blobs(centers=3, n_samples=500, n_features=2, shuffle=True, random_state=40)\n",
        "\n",
        "def euclidean_distance(x1,x2):\n",
        "  return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "class KMeans:\n",
        "  def __init__(self, K=5, max_iters=100, plot_steps=False):\n",
        "    self.K = K\n",
        "    self.max_iters = max_iters\n",
        "    self.plot_steps = plot_steps\n",
        "\n",
        "    #list of samples endices for each cluster\n",
        "    self.clusters = [[] for _ in range(self.K)]\n",
        "\n",
        "    #mean feature vector for each cluster\n",
        "    self.centroids = []\n",
        "\n",
        "  def predict(self, X):\n",
        "    self.X = X\n",
        "    self.n_samples, self.n_features = X.shape\n",
        "\n",
        "    #initialize centroids\n",
        "    random_sample_idxs = np.random.choice(self.n_samples, self.K, replace = False)\n",
        "    self.centroids = [self.X[idx] for idx in random_sample_idxs]\n",
        "\n",
        "    #optimization\n",
        "    for _ in range(self.max_iters):\n",
        "      #update clusters\n",
        "      self.clusters = self._create_clusters(self.centroids)\n",
        "      if self.plot_steps:\n",
        "        self.plot()\n",
        "\n",
        "      #update centroids\n",
        "      centroids_old = self.centroids\n",
        "      self.centroids = self._get_centroids(self.clusters)\n",
        "\n",
        "\n",
        "      #check if converged\n",
        "      if self._is_converged(centroids_old, self.centroids):\n",
        "        break\n",
        "\n",
        "      if self.plot_steps:\n",
        "        self.plot()\n",
        " \n",
        "    #return cluster labels\n",
        "    return self._get_cluster_labels(self.clusters)\n",
        "\n",
        "  def _get_cluster_labels(self, clusters):\n",
        "    labels = np.empty(self.n_samples)\n",
        "    for cluster_idx, cluster in enumerate(clusters):\n",
        "      for sample_idx in cluster:\n",
        "        labels[sample_idx] = cluster_idx\n",
        "    return labels\n",
        "\n",
        "\n",
        "  def _create_clusters(self, centroids):\n",
        "    clusters = [[] for _ in range(self.K)]\n",
        "    for idx, sample in enumerate(self.X):\n",
        "      centroid_idx = self._closest_centroid(sample, centroids)\n",
        "      clusters[centroid_idx].append(idx)\n",
        "    return clusters\n",
        "\n",
        "  def _closest_centroid(self, sample, centroids):\n",
        "    distances = [euclidean_distance(sample,point) for point in centroids]\n",
        "    closest_idx = np.argmin(distances)\n",
        "    return closest_idx\n",
        "  \n",
        "  def _get_centroids(self, clusters):\n",
        "    centroids = np.zeros((self.K, self.n_features))\n",
        "    for cluster_idx, cluster in enumerate(clusters):\n",
        "      cluster_mean = np.mean(self.X[cluster], axis=0)\n",
        "      centroids[cluster_idx] = cluster_mean\n",
        "    return centroids\n",
        "  \n",
        "  def _is_converged(self, centroids_old, centroids):\n",
        "    distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)]\n",
        "    return sum(distances) == 0\n",
        "  \n",
        "  def plot(self):\n",
        "    fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "    for i, index in enumerate(self.clusters):\n",
        "      point = self.X[index].T\n",
        "      ax.scatter(*point)\n",
        "    for point in self.centroids:\n",
        "      ax.scatter(*point,marker='x',color='black',linewidth=2)\n",
        "\n",
        "    plt.show()  \n",
        "\n",
        "clusters = len(np.unique(y))\n",
        "print(clusters)\n",
        "km = KMeans(K= clusters, max_iters=150, plot_steps=False)\n",
        "y_pred = km.predict(X)\n",
        "km.plot()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}