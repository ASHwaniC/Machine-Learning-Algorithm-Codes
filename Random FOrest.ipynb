{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Palpax Online Assessment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNS1ljnp83G0SEWGMyg9PEN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASHwaniC/Machine-Learning-Algorithm-Codes/blob/main/Random%20FOrest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0klMQ0DIS7tp"
      },
      "source": [
        "Random Forest\n",
        "\n",
        "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.Random decision forests correct for decision trees' habit of overfitting to their training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvEICt8Jznx"
      },
      "source": [
        "def bootstrap_sample(X, y):\n",
        "  n_samples = X.shape[0]\n",
        "  idxs = np.random.choice(n_samples, size = n_samples, replace=True)\n",
        "  return(X[idxs], y[idxs])\n",
        "\n",
        "def most_common_label(y):\n",
        "  counter = Counter(y)\n",
        "  most_common = counter.most_common(1)[0][0]\n",
        "  return(most_common)\n",
        "\n",
        "class RandomForest:\n",
        "  def __init__(self, n_trees = 100, min_samples_split=2, max_depth=100, n_feats=None):\n",
        "    self.n_trees = n_trees\n",
        "    self.min_samples_split = min_samples_split\n",
        "    self.max_depth = max_depth\n",
        "    self.n_feats = n_feats\n",
        "    self.trees = []\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.trees = []\n",
        "    for _ in range(self.n_trees):\n",
        "      tree = DecisionTree(min_samples_split = self.min_samples_split, max_depth=self.max_depth, n_feats=self.n_feats)\n",
        "      X_sample, y_sample = bootstrap_sample(X, y)\n",
        "      tree.fit(X_sample, y_sample)\n",
        "      self.trees.append(tree)\n",
        "  def predict(self, X):\n",
        "    tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "    tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
        "\n",
        "    y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n",
        "    return np.array(y_pred)\n",
        "\n",
        "\n",
        "clf_f = RandomForest(n_trees=3)\n",
        "clf_f.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf_f.predict(X_test)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \",acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}